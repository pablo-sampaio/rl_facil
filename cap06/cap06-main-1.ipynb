{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Cap√≠tulo 6 - Algoritmos com Modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgUyA2Di5J_n"
      },
      "source": [
        "Voc√™ pode rodar este notebook localmente ou no Colab. Para abrir diretamente no Colab, basta clicar no link abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap06/cap06-main-1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## Configura√ß√µes Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg freeglut3-dev xvfb\n",
        "\n",
        "    !pip install gym==0.23.1\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0vjjot4zl77"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # Set up fake display; otherwise rendering will fail\n",
        "    import os\n",
        "    os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "    from util.notebook import display_videos_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from util.experiments import repeated_exec, repeated_exec_greedy_Q\n",
        "from util.plot import plot_result, plot_multiple_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWAnQVF2zl7-"
      },
      "outputs": [],
      "source": [
        "VERBOSE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15104fKzl7_"
      },
      "source": [
        "## 1 - Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-uBfwRDzl8A"
      },
      "source": [
        "Segue o mesmo **Q-learning** que vimos antes (ainda com o crit√©rio de parada baseado na quantidade de epis√≥dios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc5RKqrpzl8E"
      },
      "outputs": [],
      "source": [
        "# Esta √© a pol√≠tica. Neste caso, escolhe uma a√ß√£o com base nos valores\n",
        "# da tabela Q, usando uma estrat√©gia epsilon-greedy.\n",
        "def epsilon_greedy(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUBGKor2zl8F"
      },
      "outputs": [],
      "source": [
        "def run_qlearning(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q com valores aleat√≥rios de -1.0 a 0.0\n",
        "    # usar o estado como √≠ndice das linhas e a a√ß√£o como √≠ndice das colunas\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada epis√≥dio, guarda sua soma de recompensas (retorno n√£o-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # executa 1 epis√≥dio completo, fazendo atualiza√ß√µes na Q-table\n",
        "        while not done:\n",
        "\n",
        "            # escolhe a pr√≥xima a√ß√£o -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a a√ß√£o, ou seja, d√° um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados n√£o-terminais -- valor m√°ximo (melhor a√ß√£o)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 epis√≥dios, imprime informa√ß√£o sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x5E-zx85J_u"
      },
      "outputs": [],
      "source": [
        "EPISODES = 500\n",
        "LR = 0.3\n",
        "GAMMA = 0.90\n",
        "EPSILON = 0.1\n",
        "\n",
        "rmax = 10.0\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "rewards, qtable = run_qlearning(env, EPISODES, LR, GAMMA, EPSILON)\n",
        "print(\"√öltimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfj6LvPI5J_u"
      },
      "outputs": [],
      "source": [
        "# Mostra um gr√°fico de passos x retornos n√£o descontados acumulados\n",
        "plot_result(rewards, rmax, cumulative=False, window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEeLJzatzl8H"
      },
      "source": [
        "## 2 - Dyna-Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO52zy36zl8H"
      },
      "source": [
        "Um algoritmo com modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk3dEaE7zl8I"
      },
      "outputs": [],
      "source": [
        "import random as rand\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFVuYF92zl8I"
      },
      "outputs": [],
      "source": [
        "def planning(model, planning_steps, Q, lr, gamma):\n",
        "    all_s_a = list(model.keys())\n",
        "    if len(all_s_a) < planning_steps:\n",
        "        samples = rand.choices(all_s_a, k=planning_steps)\n",
        "    else:\n",
        "        samples = rand.sample(all_s_a, k=planning_steps)\n",
        "\n",
        "    for s, a in samples:\n",
        "        r, next_s, is_terminal = model[(s,a)]\n",
        "        if is_terminal:\n",
        "            V_next_s = 0\n",
        "        else:\n",
        "            V_next_s = np.max(Q[next_s])\n",
        "        delta = (r + gamma * V_next_s) - Q[s,a]\n",
        "        Q[s,a] = Q[s,a] + lr * delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVmOdi4zzl8J"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Dyna Q\n",
        "def run_dyna_q(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, planning_steps=5):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "   # inicializa a tabela Q\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))\n",
        "\n",
        "    model = dict({})  # keys are pairs (state, action)\n",
        "\n",
        "    # para cada epis√≥dio, guarda sua soma de recompensas (retorno n√£o-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # executa 1 epis√≥dio completo, fazendo atualiza√ß√µes na Q-table\n",
        "        while not done:\n",
        "\n",
        "            # escolhe a pr√≥xima a√ß√£o -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a a√ß√£o, ou seja, d√° um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "                next_state = env.reset()\n",
        "            else:\n",
        "                # para estados n√£o-terminais -- valor m√°ximo (melhor a√ß√£o)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table / direct RL\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            # atualiza o modelo\n",
        "            model[state,action] = (reward, next_state, done)\n",
        "\n",
        "            # indirect RL / planejamento\n",
        "            planning(model, planning_steps, Q, lr, gamma)\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 1000 passos, imprime informa√ß√£o sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 1000 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Step {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    state = env.reset()\n",
        "    reward = 0\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCdswbdqzl8K"
      },
      "outputs": [],
      "source": [
        "EPISODES = 500\n",
        "LR = 0.3\n",
        "GAMMA = 0.90\n",
        "EPSILON = 0.1\n",
        "\n",
        "rmax = 10.0\n",
        "\n",
        "rewards, qtable = run_dyna_q(env, EPISODES, LR, GAMMA, EPSILON, 2)\n",
        "print(\"√öltimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxeaJNc2zl8K"
      },
      "outputs": [],
      "source": [
        "# Mostra um gr√°fico de passos x retornos n√£o descontados acumulados\n",
        "plot_result(rewards, rmax, cumulative=False, window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8roKzCgsuCl"
      },
      "source": [
        "## 3 - Experimentos com \"Taxi-v3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDfuPn7azl8L"
      },
      "source": [
        "### 3.1 - Otimizando Par√¢metros do Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqXaA24Pzl8L"
      },
      "outputs": [],
      "source": [
        "RUNS_PER_TRIAL = 3\n",
        "EPISODES_PER_TRIAL = 200\n",
        "ENV = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU8DNpphvcRa"
      },
      "outputs": [],
      "source": [
        "def train(trial : optuna.Trial):\n",
        "    # chama os m√©todos do \"trial\" (tentativa) para sugerir valores para os par√¢metros\n",
        "    lr = trial.suggest_uniform('lr', 0.1, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    gamma = trial.suggest_uniform('gamma', 0.5, 1.0)\n",
        "\n",
        "    print(f\"\\nTRIAL #{trial.number}: lr={lr}, eps={eps}, gamma={gamma}\")\n",
        "\n",
        "    # roda o algoritmo v√°rias vezes\n",
        "    results = repeated_exec(RUNS_PER_TRIAL, \"qlearn-optuna\", run_qlearning, ENV, EPISODES_PER_TRIAL, lr=lr, epsilon=eps, gamma=gamma)\n",
        "\n",
        "    # soma dos retornos n√£o-descontado finais (dos √∫ltimos 20 epis√≥dios)\n",
        "    #return np.sum(results[1][:,-1])\n",
        "    return np.sum(results[1][:,-20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVBjiirtOUP"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_planning.db',\n",
        "                        study_name=f'qlearning_taxi',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(train, n_trials=100)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PAR√ÇMETROS:\")\n",
        "print(study.best_params)\n",
        "qlearn_params_taxi = study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ-IKYWszl8M"
      },
      "source": [
        "### 3.2 - Experimentos Comparativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYSG9xiHxBKe"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"Taxi-v3\")\n",
        "EPISODES = 1_000\n",
        "RUNS = 20\n",
        "AUTO_LOAD = True\n",
        "\n",
        "results_t = []\n",
        "\n",
        "results_t.append( repeated_exec(RUNS, f\"Q-Learning \", run_qlearning, environment, EPISODES, **qlearn_params_taxi, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 1\n",
        "results_t.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passo)\", run_dyna_q, environment, EPISODES, **qlearn_params_taxi, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 10\n",
        "results_t.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passos)\", run_dyna_q, environment, EPISODES, **qlearn_params_taxi, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgbhKstd8iNB"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_t, cumulative='avg', x_log_scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLE-Ehfhzl8N"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_t, cumulative='no', x_log_scale=False, window=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9fggfZzFVV"
      },
      "source": [
        "## 4 - Experimentos com \"FrozenLake\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfDdxxuJzl8O"
      },
      "source": [
        "### 4.1 - Otimizando Par√¢metro do Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1dyVqhczl8O"
      },
      "outputs": [],
      "source": [
        "RUNS_PER_TRIAL = 7\n",
        "EPISODES_PER_TRIAL = 1000\n",
        "ENV = gym.make(\"FrozenLake-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzQXmLPPzl8O"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_planning.db',\n",
        "                        study_name=f'qlearning_frozen_',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(train, n_trials=50)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PAR√ÇMETROS:\")\n",
        "print(study.best_params)\n",
        "qlearn_params_frozen = study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vTgo1ODzl8P"
      },
      "source": [
        "### 4.2 - Experimentos Comparativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9g5jvhGzl8P"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"FrozenLake-v1\")\n",
        "EPISODES = 7_000\n",
        "RUNS = 20\n",
        "AUTO_LOAD = True\n",
        "\n",
        "results_f = []\n",
        "\n",
        "results_f.append( repeated_exec(RUNS, f\"Q-Learning \", run_qlearning, environment, EPISODES, **qlearn_params_frozen, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 1\n",
        "results_f.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passo)\", run_dyna_q, environment, EPISODES, **qlearn_params_frozen, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 3\n",
        "results_f.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passo)\", run_dyna_q, environment, EPISODES, **qlearn_params_frozen, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7H6WAj9zl8P"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_f, cumulative='avg', x_log_scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_results(results_f, cumulative='no', x_log_scale=False, window=100)"
      ],
      "metadata": {
        "id": "Yzf8o9ea27Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqpKymDJzl8Q"
      },
      "source": [
        "### 4.3 - Desafio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LpaQbBzl8Q"
      },
      "source": [
        "Parece que o Dyna-Q n√£o foi t√£o bem no ambiente `Taxi-v3`, n√£o √©? üôÅ\n",
        "\n",
        "**Por qu√™ voc√™ acha que isso aconteceu?**\n",
        "\n",
        "Proponha uma modifica√ß√£o simples no modelo do Dyna-Q para melhorar o desempenho dele nesse ambiente. Basta mudar a forma de representar o modelo!\n",
        "\n",
        "Depois, refa√ßa os experimentos da se√ß√£o 4.2 para conferir se deu certo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rymv-G0Szl8Q"
      },
      "outputs": [],
      "source": [
        "def planning_new(model, planning_steps, Q, lr, gamma):\n",
        "    # altere aqui !!!\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4B7DBrQzl8W"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Dyna Q\n",
        "def run_dyna_q_new(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, planning_steps=5):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "   # inicializa a tabela Q\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))\n",
        "\n",
        "    model = dict({})  # keys are pairs (state, action)\n",
        "\n",
        "    # para cada epis√≥dio, guarda sua soma de recompensas (retorno n√£o-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # executa 1 epis√≥dio completo, fazendo atualiza√ß√µes na Q-table\n",
        "        while not done:\n",
        "\n",
        "            # escolhe a pr√≥xima a√ß√£o -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a a√ß√£o, ou seja, d√° um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "                next_state = env.reset()\n",
        "            else:\n",
        "                # para estados n√£o-terminais -- valor m√°ximo (melhor a√ß√£o)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table / direct RL\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            # atualiza o modelo\n",
        "            # ALTERE AQUI !!!\n",
        "\n",
        "            # indirect RL / planejamento\n",
        "            planning_new(model, planning_steps, Q, lr, gamma)\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 1000 passos, imprime informa√ß√£o sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 1000 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Step {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    state = env.reset()\n",
        "    reward = 0\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSDfDQMNzl8X"
      },
      "outputs": [],
      "source": [
        "results_f2 = []\n",
        "\n",
        "results_f2.append( repeated_exec(RUNS, f\"Q-Learning \", run_qlearning, environment, EPISODES, **qlearn_params_frozen, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 1\n",
        "results_f2.append( repeated_exec(RUNS, f\"Dyna-Q New ({plan_steps} passos)\", run_dyna_q_new, environment, EPISODES, **qlearn_params_frozen, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 3\n",
        "results_f2.append( repeated_exec(RUNS, f\"Dyna-Q New ({plan_steps} passos)\", run_dyna_q_new, environment, EPISODES, **qlearn_params_frozen, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK1NJfrhzl8X"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_f2, cumulative='avg', x_log_scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w8GPbGNzl8Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "cap06-main-1.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}