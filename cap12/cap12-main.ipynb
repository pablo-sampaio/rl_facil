{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap12/cap12-main.ipynb)\n",
        "\n",
        "# Capítulo 12 - Tarefas Continuadas: Formulação Alternativa\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    #clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from envs import TwoChoiceEnv, AccessControlEnv\n",
        "from envs.wrappers import FromDiscreteTupleToDiscreteObs\n",
        "\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "from util.qtable_helper import epsilon_greedy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U-xEwtye5J_r"
      },
      "source": [
        "## 1 - Tarefa Continuada (Infinita)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRc8KisnAWV"
      },
      "source": [
        "Uma **tarefa continuada** (*continuing task*) é uma tarefa representada por um MDP sem estado terminal (que pode ser chamado de *MDP continuado* ou *MDP de horizonte infinito*).\n",
        "\n",
        "Existem várias críticas quanto à aplicação de algoritmos baseados em *retornos descontados* nestes ambientes, conforme o artigo \"*Discounted Reinforcement Learning is Not an Optimization Problem*\" (Naik et al., 2019).\n",
        "\n",
        "O artigo propõe o MDP abaixo para ilustrar as dificuldades de métodos como o *Q-Learning* e o *SARSA* em tarefas desse tipo:\n",
        "\n",
        "<p align=\"center\">\n",
        "   <img src=\"https://github.com/pablo-sampaio/rl_facil/raw/main/cap12/two-choice-mdp-naik2019.jpg\" alt=\"The Two-choice MDP (Naik et al., 2019)\" width=\"500\">\n",
        "</p?>\n",
        "\n",
        "Este MDP está implementado como ambiente `gym` na classe `util.env.TwoChoiceEnv`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nMESVFHanAWW"
      },
      "source": [
        "## 2 - Q-Learning (parando por passos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WLSTliTXnAWX"
      },
      "source": [
        "Abaixo, segue o código do **Q-learning**, com uma pequena alteração em relação à implementação dada antes:\n",
        "- o critério de parada agora é a *quantidade de passos*\n",
        "- não importa a quantidade de episódios envolvida.\n",
        "\n",
        "Este será o critério de parada na maioria dos algoritmos que veremos no futuro.\n",
        "\n",
        "*Atenção: alguns gráficos vão mostrar o eixo \"x\" como \"episódios\", mas entenda como \"passos\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D3s-gMMKnAWX"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Q-learning\n",
        "def run_qlearning_step(env, total_steps, lr=0.1, gamma=0.95, epsilon=0.1, verbose=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low=-0.01, high=0.01, size=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # guarda a recompensa de cada passo\n",
        "    rewards_per_step = []\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    reward = 0\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(total_steps):\n",
        "\n",
        "        # escolhe a próxima ação -- usa epsilon-greedy\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        # realiza a ação, ou seja, dá um passo no ambiente\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            # para estados terminais\n",
        "            V_next_state = 0\n",
        "        else:\n",
        "            # para estados não-terminais -- valor máximo (melhor ação)\n",
        "            V_next_state = np.max(Q[next_state,:])\n",
        "\n",
        "        # atualiza a Q-table\n",
        "        # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "        delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "        Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "        rewards_per_step.append(reward)\n",
        "\n",
        "        if done:\n",
        "            state, _ = env.reset()\n",
        "        else:\n",
        "            state = next_state\n",
        "\n",
        "        # a cada 1000 passos, imprime informação sobre o progresso\n",
        "        if verbose and ((i+1) % 1000 == 0):\n",
        "            avg_reward = np.mean(rewards_per_step[-100:])\n",
        "            print(f\"Step {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return rewards_per_step, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x5E-zx85J_u"
      },
      "outputs": [],
      "source": [
        "TOTAL_STEPS = 1_000\n",
        "LR = 0.2\n",
        "GAMMA = 0.70   # só deve dar a política ótima para valores a partir de 0.85\n",
        "EPSILON = 0.1\n",
        "\n",
        "rmax = TOTAL_STEPS//2\n",
        "env = TwoChoiceEnv()\n",
        "\n",
        "rewards1, qtable1 = run_qlearning_step(env, TOTAL_STEPS, LR, GAMMA, EPSILON, verbose=True)\n",
        "print(\"Acumulado final =\", sum(rewards1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfj6LvPI5J_u"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de passos x retornos não descontados acumulados\n",
        "plot_result(rewards1, rmax, cumulative='avg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos conferir o valor (retorno esperado) de cada ação no estado 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qtable1[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hJE9NfkKnAWY"
      },
      "source": [
        "## 3 - Differential Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eGfHyzTtnAWY"
      },
      "source": [
        "Um algoritmo específico para tarefas continuadas.\n",
        "\n",
        "Ele é baseado na **formulação de recompensa média** para as tarefas (e os MDPs). Nessa formulação:\n",
        "- é usado um *retorno* baseado nas diferenças entre cada recompensas real $R_t$ e a recompensa média até o passo $t$\n",
        "- não existe o fator de desconto $\\gamma$ (gamma)\n",
        "- mas há um novo parâmetro $\\eta$ (êta) que controla a \"taxa de aprendizagem\" da recompensa média estimada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LvHlt_CunAWY"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Differential Q-learning\n",
        "def run_differential_qlearning_step(env, total_steps, lr=0.1, lr_mean=0.1, epsilon=0.1, verbose=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low=-0.01, high=0.01, size=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # guarda a recompensa de cada passo\n",
        "    rewards_per_step = []\n",
        "    states = []\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    reward = 0\n",
        "    mean_reward = 0.0\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(total_steps):\n",
        "\n",
        "        # escolhe a próxima ação -- usa epsilon-greedy\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        # realiza a ação, ou seja, dá um passo no ambiente\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        assert not done, \"This algorithm is for infinite tasks!\"\n",
        "\n",
        "        # valor do próximo estado - valor máximo (melhor ação)\n",
        "        V_next_state = np.max(Q[next_state,:])\n",
        "\n",
        "        # atualiza a Q-table\n",
        "        # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "        delta = (reward - mean_reward + V_next_state) - Q[state,action]\n",
        "        Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "        # atualiza a recompensa média\n",
        "        mean_reward += lr_mean * delta\n",
        "        # alt.: mean_reward += lr_mean * (reward - mean_reward) # maior variância\n",
        "\n",
        "        states.append(state)\n",
        "        rewards_per_step.append(reward)\n",
        "        state = next_state\n",
        "\n",
        "        # a cada 1000 passos, imprime informação sobre o progresso\n",
        "        if verbose and ((i+1) % 1000 == 0):\n",
        "            avg_reward = np.mean(rewards_per_step[-100:])\n",
        "            print(f\"Step {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return rewards_per_step, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq2xEWrSnAWZ"
      },
      "outputs": [],
      "source": [
        "TOTAL_STEPS = 10_000\n",
        "LR      = 0.4\n",
        "LR_MEAN = 0.01\n",
        "EPSILON = 0.1\n",
        "\n",
        "rmax = TOTAL_STEPS//2\n",
        "env = TwoChoiceEnv(coherent_action=False)\n",
        "\n",
        "rewards2, qtable2 = run_differential_qlearning_step(env, TOTAL_STEPS, LR, LR_MEAN, EPSILON, verbose=True)\n",
        "print(\"Acumulado final =\", sum(rewards2))\n",
        "\n",
        "assert np.isnan(qtable2).sum() == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Valores das ações no estado **0**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qtable2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx-4ChcInAWZ"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de passos x retornos não descontados acumulados\n",
        "plot_result(rewards2, rmax, cumulative='avg')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M8roKzCgsuCl"
      },
      "source": [
        "## 4 - Otimizando Parâmetros"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hG_9-nZ-UDqu"
      },
      "source": [
        "Vamos usar a biblioteca *Optuna* para otimizar os (hiper-)parâmetros dos algoritmos de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tfPiRKkFnAWZ"
      },
      "outputs": [],
      "source": [
        "ENV = TwoChoiceEnv()\n",
        "#environment = FromDiscreteTupleToDiscreteObs( AccessControlEnv() )\n",
        "\n",
        "RUNS_PER_TRIAL = 10\n",
        "TRAIN_STEPS = 500"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MBsJRMCstj0N"
      },
      "source": [
        "### 4.1 - Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pM4P7CdwUDqv"
      },
      "source": [
        "Vamos rodar duas otimizações separadas para estes dois valores de `gamma`: `0.7` e `0.9`.\n",
        "\n",
        "Espera-se que o segundo valor (`0.9`) permita o Q-Learning chegar a um resultado ótimo, mas o primeiro valor (`0.7`) não."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aU8DNpphvcRa"
      },
      "outputs": [],
      "source": [
        "def create_train_fn(fixed_gamma):\n",
        "    def train(trial : optuna.Trial):\n",
        "        # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "        lr = trial.suggest_float('lr', 0.01, 1.0)\n",
        "        eps = trial.suggest_float('epsilon', 0.01, 1.00)\n",
        "\n",
        "        print(f\"\\nTRIAL #{trial.number}: lr={lr}, eps={eps}\")\n",
        "\n",
        "        # roda o algoritmo várias vezes\n",
        "        results = repeated_exec(RUNS_PER_TRIAL, \"qlearn-optuna\", run_qlearning_step, ENV, TRAIN_STEPS, lr=lr, epsilon=eps, gamma=fixed_gamma)\n",
        "\n",
        "        # média das somas das recompensas de cada treinamento\n",
        "        # results[1] -> guarda as listas de recompensas dos vários treinamentos, com shape (treinamentos x passos)\n",
        "        return results[1].sum(axis=1).mean()\n",
        "    return train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVBjiirtOUP"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.70\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_continuing.db',\n",
        "                        study_name=f'qlearning-g{GAMMA}',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(create_train_fn(GAMMA), n_trials=50)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PARÂMETROS PARA GAMMA\", GAMMA, \":\")\n",
        "print(study.best_params)\n",
        "qlearn_params_g07 = study.best_params\n",
        "qlearn_params_g07['gamma'] = GAMMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcVFWiM0nAWa"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.90\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_continuing.db',\n",
        "                        study_name=f'qlearning-g{GAMMA}',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(create_train_fn(GAMMA), n_trials=50)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PARÂMETROS PARA GAMMA\", GAMMA, \":\")\n",
        "print(study.best_params)\n",
        "qlearn_params_g09 = study.best_params\n",
        "qlearn_params_g09['gamma'] = GAMMA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_N6eLcX7yfqd"
      },
      "source": [
        "### 4.2 - Differential Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T_6xX6d_0lht"
      },
      "outputs": [],
      "source": [
        "def train_diff(trial : optuna.Trial):\n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    lr = trial.suggest_float('lr', 0.01, 1.0)\n",
        "    eps = trial.suggest_float('epsilon', 0.01, 1.00)\n",
        "    lrmean = trial.suggest_float('lr_mean', 0.01, 1.0)\n",
        "\n",
        "    print(f\"\\nTRIAL #{trial.number}: {lr=}, {eps=}, {lrmean=}\")\n",
        "\n",
        "    # roda o algoritmo várias vezes\n",
        "    results = repeated_exec(RUNS_PER_TRIAL, \"diff-qlearn-optuna\", run_differential_qlearning_step, ENV, TRAIN_STEPS, lr=lr, epsilon=eps, lr_mean=lrmean)\n",
        "\n",
        "    # média das somas das recompensas de cada treinamento\n",
        "    return results[1].sum(axis=1).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5YmxWYRnAWb"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_continuing.db',\n",
        "                        study_name='diff-qlearning',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(train_diff, n_trials=100)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PARÂMETROS:\")\n",
        "print(study.best_params)\n",
        "diff_qlearn_params = study.best_params"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9fggfZzFVV"
      },
      "source": [
        "## 5 - Experimentos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iCSZcn-inAWb"
      },
      "source": [
        "### 5.1 - Desempenho no Treinamento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6eQTwBv0nAWb"
      },
      "source": [
        "Comparando os dois usando os parâmetros ótimos obtidos antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jYSG9xiHxBKe"
      },
      "outputs": [],
      "source": [
        "TRAIN_STEPS = 1_000\n",
        "RUNS = 200\n",
        "\n",
        "results1 = []\n",
        "\n",
        "results1.append( repeated_exec(RUNS, f\"Diff Q-Learning\", run_differential_qlearning_step, ENV, TRAIN_STEPS, **diff_qlearn_params) )\n",
        "clear_output()\n",
        "\n",
        "results1.append( repeated_exec(RUNS, f\"Q-Learning (g=0.7)\", run_qlearning_step, ENV, TRAIN_STEPS, **qlearn_params_g07) )\n",
        "clear_output()\n",
        "\n",
        "results1.append( repeated_exec(RUNS, f\"Q-Learning (g=0.9)\", run_qlearning_step, ENV, TRAIN_STEPS, **qlearn_params_g09) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgbhKstd8iNB"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results1, cumulative='sum', window=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmEt5JFvnAWb"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results1, cumulative='sum', window=1, plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yjS2ZsvknAWc"
      },
      "outputs": [],
      "source": [
        "#index = 0\n",
        "#plot_multiple_results(results1[index:index+1], cumulative='sum', window=1, plot_stddev=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W_0Gzkc1nAWc"
      },
      "source": [
        "### 5.2 - Desempenho Pós-Treinamento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s-1j7zGDsJWy"
      },
      "source": [
        "Os experimentos abaixo avaliam o agente após o treinamento. Para isso, o loop treinamento-avaliação é repetido várias vezes.\n",
        "\n",
        "Surpreendentemente, os resultados variam muito. Mas, espera-se que, no ambiente `TwoChoiceEnv`: \n",
        "- o *Differential Q-Learning* e o *Q-Learning* com gammas altos (como `0.9`) atinjam o ótimo, \n",
        "- mas o *Q-Learning* com valor de gamma baixo (como `0.7`) não atinja o ótimo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from util.qtable_helper import evaluate_qtable_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OJWntkusnAWc"
      },
      "outputs": [],
      "source": [
        "TRAIN_STEPS = 1_000\n",
        "TEST_STEPS = 100\n",
        "REPETITIONS = 300\n",
        "\n",
        "results_dict = { \"Diff-QLearn-Greedy\": np.zeros(shape=(REPETITIONS,)),\n",
        "                 \"QLearn(0.7)-Greedy\": np.zeros(shape=(REPETITIONS,)),\n",
        "                 \"QLearn(0.9)-Greedy\": np.zeros(shape=(REPETITIONS,)) }\n",
        "\n",
        "for i in range(REPETITIONS):\n",
        "    print(f\"({i}) Treinando o Differential Q-Learning\")\n",
        "    _, qtable1 = run_differential_qlearning_step(ENV, TRAIN_STEPS, **diff_qlearn_params, verbose=False)\n",
        "\n",
        "    print(f\"({i}) Treinando o Q-Learning (g=0.7)\")\n",
        "    _, qtable2 = run_qlearning_step(ENV, TRAIN_STEPS, **qlearn_params_g07, verbose=False)\n",
        "\n",
        "    print(f\"({i}) Treinando o Q-Learning (g=0.9)\")\n",
        "    _, qtable3 = run_qlearning_step(ENV, TRAIN_STEPS, **qlearn_params_g09, verbose=False)\n",
        "\n",
        "    print(\"------ \")\n",
        "    print(f\"({i}) Executando políticas 'greedy' com as Q-tables treinadas:\")\n",
        "    \n",
        "    mean_reward, _ = evaluate_qtable_policy(ENV, qtable1, num_episodes=1)\n",
        "    results_dict[\"Diff-QLearn-Greedy\"][i] = mean_reward\n",
        "    \n",
        "    mean_reward, _ = evaluate_qtable_policy(ENV, qtable2, num_episodes=1)\n",
        "    results_dict[\"QLearn(0.7)-Greedy\"][i] = mean_reward\n",
        "    \n",
        "    mean_reward, _ = evaluate_qtable_policy(ENV, qtable3, num_episodes=1)\n",
        "    results_dict[\"QLearn(0.9)-Greedy\"][i] = mean_reward\n",
        "\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extract keys and calculate means and stds\n",
        "algorithms = results_dict.keys()\n",
        "means = [np.mean(results_dict[x]) for x in algorithms]\n",
        "stds = [np.std(results_dict[x]) for x in algorithms]\n",
        "\n",
        "# Create bar plot with error bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(algorithms, means, yerr=stds, capsize=5)\n",
        "\n",
        "# Customize the plot\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Recompensa (Média)')\n",
        "plt.title('Desempenho Médio Após Treinamentos, com Desvio Padrão')\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "means, stds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 - Desafio"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cada um dos algoritmos básicos de TD-Learning da formulação padrão (com retorno descontado) tem um correspondente na formulação de recompensa média.\n",
        "\n",
        "O **Differential Q-Learning**, que vimos aqui, é o correspondente do *Q-Learning*.\n",
        "\n",
        "O desafio é este:\n",
        "1. Implementar um **Differential SARSA**\n",
        "1. Otimizar seus parâmetros\n",
        "1. Rodar experimentos comparando-o com os algoritmos anteriores\n",
        "\n",
        "Também destacamos que, no livro de Sutton & Barto tem uma implementação do **Differential SARSA de n passos** usando *aproximação de função*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqxKGeh4nAWk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "cap12-main.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "rl23",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
